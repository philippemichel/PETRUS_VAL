---
title: "Inter-observer Validation Study: Ultrasound Reading Quality"
subtitle: "PETRUS_VAL Study"
author: "Philippe MICHEL"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    theme: cosmo
    highlight: tango
    code_folding: hide
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6
)

# Load required packages
library(tidyverse)
library(irr)
library(psych)
library(ggplot2)
library(knitr)
library(kableExtra)

# Set ggplot theme
theme_set(theme_minimal())
```

# Executive Summary

This report presents the results of an inter-observer validation study examining the agreement between multiple observers in assessing ultrasound reading quality.

**Key Findings:**
- [To be completed after analysis]

# Introduction

## Background

Ultrasound imaging is a critical diagnostic tool in clinical practice. The interpretation and quality assessment of ultrasound readings require expertise and can vary between observers. This study evaluates inter-observer reliability to ensure consistent quality standards.

## Objectives

**Primary Objective:** Evaluate inter-observer agreement in ultrasound reading quality assessment.

**Secondary Objectives:**
- Identify areas of strong and weak agreement
- Assess the need for standardized training
- Establish quality benchmarks

## Methods

### Study Design
- Inter-observer validation study
- Multiple trained observers independently rate ultrasound readings
- Blinded assessment

### Statistical Analysis
- Cohen's Kappa / Fleiss' Kappa for categorical variables
- Intraclass Correlation Coefficient (ICC) for continuous variables
- Bland-Altman plots for agreement visualization

# Data Overview

```{r load-data}
# Load data
# Replace with your actual data file
# data <- read.csv("../data/data.csv")

# For demonstration purposes, create example data
set.seed(123)
n_readings <- 30
n_observers <- 3

data <- expand.grid(
  reading_id = sprintf("US%03d", 1:n_readings),
  observer_id = sprintf("OBS%d", 1:n_observers)
) %>%
  mutate(
    technical_quality = sample(c("Poor", "Fair", "Good", "Excellent"), 
                               n(), replace = TRUE, 
                               prob = c(0.1, 0.2, 0.4, 0.3)),
    diagnostic_quality = sample(c("Poor", "Fair", "Good", "Excellent"), 
                                n(), replace = TRUE, 
                                prob = c(0.1, 0.2, 0.4, 0.3)),
    overall_quality = round(rnorm(n(), mean = 7, sd = 1.5), 1),
    overall_quality = pmin(pmax(overall_quality, 1), 10),
    adequacy = sample(c("Yes", "No"), n(), replace = TRUE, prob = c(0.85, 0.15))
  )
```

```{r data-summary}
# Data summary
cat("Number of ultrasound readings:", length(unique(data$reading_id)), "\n")
cat("Number of observers:", length(unique(data$observer_id)), "\n")
cat("Total observations:", nrow(data), "\n")
```

## Data Structure

```{r data-structure}
kable(head(data, 10), caption = "First 10 rows of data") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

## Descriptive Statistics

```{r descriptive-stats}
# Summary by observer
summary_stats <- data %>%
  group_by(observer_id) %>%
  summarise(
    n_readings = n(),
    mean_quality = mean(overall_quality, na.rm = TRUE),
    sd_quality = sd(overall_quality, na.rm = TRUE),
    median_quality = median(overall_quality, na.rm = TRUE),
    pct_adequate = sum(adequacy == "Yes", na.rm = TRUE) / n() * 100
  )

kable(summary_stats, digits = 2, 
      caption = "Descriptive statistics by observer") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

# Inter-observer Agreement Analysis

## Categorical Variables

### Technical Quality

```{r technical-quality-kappa}
# Reshape data for technical quality
tech_wide <- data %>%
  select(reading_id, observer_id, technical_quality) %>%
  pivot_wider(names_from = observer_id, values_from = technical_quality)

tech_matrix <- as.matrix(tech_wide[, -1])

# Calculate Fleiss' Kappa
tech_kappa <- kappam.fleiss(tech_matrix)

cat("Fleiss' Kappa for Technical Quality:", round(tech_kappa$value, 3), "\n")
cat("P-value:", format.pval(tech_kappa$p.value), "\n")
```

**Interpretation:** `r if(tech_kappa$value < 0.40) {"Fair agreement"} else if(tech_kappa$value < 0.60) {"Moderate agreement"} else if(tech_kappa$value < 0.80) {"Good agreement"} else {"Excellent agreement"}`

```{r technical-quality-table}
# Frequency table
tech_table <- table(data$observer_id, data$technical_quality)
kable(tech_table, caption = "Technical Quality ratings by observer") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### Diagnostic Quality

```{r diagnostic-quality-kappa}
# Reshape data for diagnostic quality
diag_wide <- data %>%
  select(reading_id, observer_id, diagnostic_quality) %>%
  pivot_wider(names_from = observer_id, values_from = diagnostic_quality)

diag_matrix <- as.matrix(diag_wide[, -1])

# Calculate Fleiss' Kappa
diag_kappa <- kappam.fleiss(diag_matrix)

cat("Fleiss' Kappa for Diagnostic Quality:", round(diag_kappa$value, 3), "\n")
cat("P-value:", format.pval(diag_kappa$p.value), "\n")
```

**Interpretation:** `r if(diag_kappa$value < 0.40) {"Fair agreement"} else if(diag_kappa$value < 0.60) {"Moderate agreement"} else if(diag_kappa$value < 0.80) {"Good agreement"} else {"Excellent agreement"}`

## Continuous Variables

### Overall Quality Score

```{r overall-quality-icc}
# Reshape data for overall quality
overall_wide <- data %>%
  select(reading_id, observer_id, overall_quality) %>%
  pivot_wider(names_from = observer_id, values_from = overall_quality)

overall_matrix <- as.matrix(overall_wide[, -1])

# Calculate ICC
overall_icc <- ICC(overall_matrix)

# Display ICC results
icc_results <- overall_icc$results[c("ICC(2,1)", "ICC(2,k)"), 
                                    c("ICC", "F", "p", "lower bound", "upper bound")]

kable(icc_results, digits = 3, 
      caption = "ICC results for Overall Quality Score") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Interpretation:** 
- ICC(2,1) = `r round(overall_icc$results["ICC(2,1)", "ICC"], 3)` (single rater): `r if(overall_icc$results["ICC(2,1)", "ICC"] < 0.40) {"Fair"} else if(overall_icc$results["ICC(2,1)", "ICC"] < 0.60) {"Moderate"} else if(overall_icc$results["ICC(2,1)", "ICC"] < 0.80) {"Good"} else {"Excellent"}` agreement
- ICC(2,k) = `r round(overall_icc$results["ICC(2,k)", "ICC"], 3)` (average rater): `r if(overall_icc$results["ICC(2,k)", "ICC"] < 0.40) {"Fair"} else if(overall_icc$results["ICC(2,k)", "ICC"] < 0.60) {"Moderate"} else if(overall_icc$results["ICC(2,k)", "ICC"] < 0.80) {"Good"} else {"Excellent"}` agreement

# Visualizations

## Distribution of Ratings

```{r rating-distributions, fig.height=8}
# Plot distributions by observer
p1 <- ggplot(data, aes(x = observer_id, fill = technical_quality)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Technical Quality Distribution by Observer",
       x = "Observer", y = "Percentage", fill = "Rating") +
  theme(legend.position = "bottom")

p2 <- ggplot(data, aes(x = observer_id, y = overall_quality, fill = observer_id)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  labs(title = "Overall Quality Score Distribution by Observer",
       x = "Observer", y = "Overall Quality Score") +
  theme(legend.position = "none")

gridExtra::grid.arrange(p1, p2, ncol = 1)
```

## Agreement Heatmap

```{r agreement-heatmap}
# Calculate pairwise percentage agreement
observers <- unique(data$observer_id)
n_obs <- length(observers)
agreement_matrix <- matrix(NA, n_obs, n_obs)
rownames(agreement_matrix) <- observers
colnames(agreement_matrix) <- observers

for (i in 1:n_obs) {
  for (j in 1:n_obs) {
    if (i == j) {
      agreement_matrix[i, j] <- 1.0
    } else {
      obs_data <- data %>%
        filter(observer_id %in% c(observers[i], observers[j])) %>%
        select(reading_id, observer_id, technical_quality) %>%
        pivot_wider(names_from = observer_id, values_from = technical_quality)
      
      agreement_matrix[i, j] <- sum(obs_data[[observers[i]]] == obs_data[[observers[j]]], 
                                     na.rm = TRUE) / nrow(obs_data)
    }
  }
}

# Convert to data frame for plotting
agreement_df <- as.data.frame(agreement_matrix) %>%
  rownames_to_column("Observer1") %>%
  pivot_longer(cols = -Observer1, names_to = "Observer2", values_to = "Agreement")

# Create heatmap
ggplot(agreement_df, aes(x = Observer1, y = Observer2, fill = Agreement)) +
  geom_tile(color = "white", size = 1) +
  geom_text(aes(label = sprintf("%.2f", Agreement)), size = 5) +
  scale_fill_gradient2(low = "#d73027", mid = "#fee090", high = "#1a9850",
                      midpoint = 0.7, limits = c(0, 1)) +
  labs(title = "Inter-observer Agreement Heatmap (Technical Quality)",
       x = "Observer", y = "Observer", fill = "Agreement") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Bland-Altman Plot

```{r bland-altman}
# Create Bland-Altman plot for first two observers
if (n_obs >= 2) {
  obs1 <- observers[1]
  obs2 <- observers[2]
  
  ba_data <- data %>%
    filter(observer_id %in% c(obs1, obs2)) %>%
    select(reading_id, observer_id, overall_quality) %>%
    pivot_wider(names_from = observer_id, values_from = overall_quality) %>%
    mutate(
      mean = (!!sym(obs1) + !!sym(obs2)) / 2,
      diff = !!sym(obs1) - !!sym(obs2)
    )
  
  mean_diff <- mean(ba_data$diff, na.rm = TRUE)
  sd_diff <- sd(ba_data$diff, na.rm = TRUE)
  upper_loa <- mean_diff + 1.96 * sd_diff
  lower_loa <- mean_diff - 1.96 * sd_diff
  
  ggplot(ba_data, aes(x = mean, y = diff)) +
    geom_point(alpha = 0.6, size = 3) +
    geom_hline(yintercept = mean_diff, color = "blue", linetype = "dashed", size = 1) +
    geom_hline(yintercept = upper_loa, color = "red", linetype = "dashed", size = 1) +
    geom_hline(yintercept = lower_loa, color = "red", linetype = "dashed", size = 1) +
    geom_hline(yintercept = 0, color = "gray", linetype = "dotted") +
    labs(title = sprintf("Bland-Altman Plot: %s vs %s", obs1, obs2),
         x = "Mean of two ratings",
         y = "Difference between ratings") +
    annotate("text", x = max(ba_data$mean, na.rm = TRUE), y = upper_loa,
             label = sprintf("Upper LoA: %.2f", upper_loa), hjust = 1, vjust = -0.5) +
    annotate("text", x = max(ba_data$mean, na.rm = TRUE), y = lower_loa,
             label = sprintf("Lower LoA: %.2f", lower_loa), hjust = 1, vjust = 1.5) +
    annotate("text", x = max(ba_data$mean, na.rm = TRUE), y = mean_diff,
             label = sprintf("Mean diff: %.2f", mean_diff), hjust = 1, vjust = -0.5)
}
```

# Discussion

## Summary of Findings

**Inter-observer Agreement:**
- Technical Quality: Kappa = `r round(tech_kappa$value, 3)`
- Diagnostic Quality: Kappa = `r round(diag_kappa$value, 3)`
- Overall Quality: ICC = `r round(overall_icc$results["ICC(2,1)", "ICC"], 3)`

## Interpretation

[To be completed based on actual results]

## Limitations

- Sample size considerations
- Observer selection and experience
- Generalizability of findings

## Recommendations

- Areas requiring standardized training
- Quality improvement initiatives
- Future validation studies

# Conclusion

This inter-observer validation study provides quantitative evidence of agreement levels between observers in assessing ultrasound reading quality. The findings support [conclusions to be drawn from actual data].

# References

1. Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics. 1977;33(1):159-174.
2. Shrout PE, Fleiss JL. Intraclass correlations: uses in assessing rater reliability. Psychol Bull. 1979;86(2):420-428.
3. Bland JM, Altman DG. Statistical methods for assessing agreement between two methods of clinical measurement. Lancet. 1986;1(8476):307-310.
4. Kottner J, et al. Guidelines for Reporting Reliability and Agreement Studies (GRRAS). J Clin Epidemiol. 2011;64(1):96-106.

---

# Appendix

## Session Information

```{r session-info}
sessionInfo()
```

## Data Quality Checks

```{r data-quality}
# Missing data summary
missing_summary <- data %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Percentage = Missing_Count / nrow(data) * 100)

kable(missing_summary, digits = 2, 
      caption = "Missing data summary") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```
